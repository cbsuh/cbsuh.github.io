---
layout: page
title: 마르코프 연쇄의 확장 - 베이지안 네트워크
permalink: /books/beauty_of_mathematics/베이지안_네트워크
lang: ko
katex: true
---

### [베이지안 네트워크][wiki-bayesian-network]

* 가중되는 유향 그래프
* belief network
* 각 node의 확률을 [베이즈 공식][wiki-bayes-theorem]으로 계산 가능
* 마르코프 연쇄의 확장
  * topological structure가 마르코프 연쇄보다 유연
    * 마르코프 연쇄의 연쇄형 구조의 구속을 받지 않음 (?)
* 학습
  * 구조 학습
    * 위상학적 구조를 얻는 과정
  * 매개변수 학습
    * 매개변수를 얻는 과정
* NP-complete 문제임
  * 특정 문제만 단순화시켜서 계산 가능
* [graphical models toolkit][gmtk_tutorial]

### 단어 분류

* 통계기반 모델의 경우 (topic model)
  * 글에서 고유벡터 추출
  * cosine 유사성으로 이 고유벡터를 주제 고유벡터에 대응
* 베이지안 네트워크를 사용하는 경우 : 구글 Rephil
  * 글, 개념(주제), 키워드의 network
  * Phil cluster를 개량했음.
    * 학습 data량을 늘림
    * 키워드의 유사성을 text중 동시 발생에서 문맥중 동시 발생으로 변경

### 베이지안 네트워크의 학습

* 구조 학습
  * 완전 탐색(exhaustive search)을 해야 global optimum이 보장됨.
    * 하지만, NP-hard
  * 예전 방법
    * greedy algorithm 사용. $$ \rightarrow $$ local optimum 문제.
    * Monte carlo algorithm 적용으로 local optimum 문제 개선
      * 난수로 local optimum인지 조사
    * 계산량이 많다.
  * 새 방법
    * 노드 2개씩 상호 정보량 계산
    * 상호 정보량이 큰 경우만 직접 연결되는 network 구성
    * 이 network를 완전 탐색
* 매개변수 학습
  * 사후확률([posterior probability][wiki-posterior-probability])가 최대가 되게
    * EM process 사용
  * 조건 $$ X $$ 와 결과 $$ Y $$ 의 근접확률 $$ P(X, Y) $$ 계산

    > 이 부분은 아직 잘 모르겠음.

    * 학습 data가 제한조건 제공
    * 학습 model이 제한조건을 충족해야 함
    * 학습 model이 최대 엔트로피 모델을 충족해야 함
      * 최대 엔트로피 관련 방법을 사용할 수 있음.
* 구조 학습과 매개변수 학습을 반복

### 참고

* Wikipedia: [Bayesian network][wiki-bayesian-network]
* Wikipedia: [Bayes' theorem][wiki-bayes-theorem]
* Wikipedia: [Posterior probability][wiki-posterior-probability]
* [GMTK Tutorial: The Graphical Models Toolkit][gmtk_tutorial]

[wiki-bayesian-network]: https://en.wikipedia.org/wiki/Bayesian_network
[wiki-bayes-theorem]: https://en.wikipedia.org/wiki/Bayes%27_theorem
[wiki-posterior-probability]: https://en.wikipedia.org/wiki/Posterior_probability
[gmtk_tutorial]: https://ptolemy.berkeley.edu/projects/terraswarm/pubs/800.html
