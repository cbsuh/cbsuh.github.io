---
layout: page
title: 코사인 법칙과 뉴스의 분류
permalink: /books/beauty_of_mathematics/코사인_법칙과_뉴스의_분류
lang: ko
katex: true
---

### 뉴스의 feature vector

* 뉴스에서 사전의 모든 단어들의 TF-IDF값을 계산한 vector
* 사전에 64000 단어가 있으면, 64000 차원 vector
* TF-IDF 값이 큰 단어가 뉴스의 주제와 관련이 크다.

* Wikipedia: [Feature (machine learning)][wiki-feature]
* Wikipedia: [tf-idf (term frequency–inverse document frequency)][wiki-tfidf]

### 두 뉴스 $$ A $$와 $$ B $$의 유사도 측정

1. 두 뉴스의 feature vector 계산.
    * 뉴스 $$ A $$의 feature vector: $$ \bold{a} $$
    * 뉴스 $$ B $$의 feature vector: $$ \bold{b} $$
1. 두 뉴스의 feature vector 사이의 $$ cos{\theta} $$ 값 계산.

    두 벡터의 내적이

    $$ \bold{a} \cdot \bold{b} = |\bold{a}| |\bold{b}| cos{\theta} $$

    이므로,

    $$ cos{\theta} = \frac{\bold{a} \cdot \bold{b}}{|\bold{a}| |\bold{b}|} $$

    * Wikipedia: [Dot product][wiki-dot_product] 참고.

1. 두 벡터의 각이 $$ 0^{\degree} $$이면 두 벡터가 같고, 각이 $$ 90^{\degree} $$이면 서로 무관하다고 볼 수 있다.
    * 유사: $$ \cos{\theta} \approx 1 $$
    * 무관: $$ \cos{\theta} = 0 $$

### 뉴스들을 분류하는 법

* 많은 학위 논문을 분야별로 전문가에게 보내기 위해서 고안됨.
* 뉴스가 $$N$$개 일 때,
    1. 뉴스를 2개씩 유사성을 계산.
    1. 유사성이 역치 이상인 것을 소분류로 합침.
        * 소분류가 $$ N_1 $$개이고, $$ N_1 < N $$.
    1. 소분류들을 하나의 뉴스처럼 feature vector를 계산.
    1. 1, 2를 반복.
        * 새로운 분류 $$ N_2 $$개를 만들면, $$ N_2 < N_1 $$.
    1. 전체 반복.

### 뉴스 분류의 시간 복잡도

* 두 vector $$ \bold{a} $$와 $$ \bold{b} $$ 의 $$ \cos{\theta} $$를 구하는 복잡도:

    $$ O({|\bold{a}| + |\bold{b}|)} $$

    항상 $$ \| \bold{a} \| > \| \bold{b} \| $$ 라고 가정하면,

    $$ O(|\bold{a}|) $$

* 뉴스 하나를 다른 $$ N $$ 개의 뉴스와 비교하는 복잡도:

    $$ O(N \cdot |\bold{a}|) $$

* 뉴스 $$ N $$개를 둘 씩 pair로 비교하는 복잡도:

    $$ O(N^2 \cdot |\bold{a}|) $$

뉴스 개수가 많으면, 계산량이 많다.

### 뉴스 분류를 Optimize 하는 법

* 분모의 계산에 필요한 feature vector의 길이를 저장.
  * 계산량 $$ \frac{2}{3} $$ 절감.
* 분자의 dot product 계산 시, 0이 아닌 원소만 계산.
  * 기사의 단어 개수가, 사전의 단어 개수보다 많이 적다.
* 허사(접속사, 부사, 전치사) 제거

### 뉴스 분류의 정확도를 올리는 법

* 제목과 첫단락/끝단락의 단어들에 가중치를 준다.
  * 학교에서 배우는 작문법이 이렇다.

#### 참고

* Wikipedia: [Feature (machine learning)][wiki-feature]
  * feature vector 설명
* Wikipedia: [tf-idf (term frequency–inverse document frequency)][wiki-tfidf]
* Wikipedia: [Dot product][wiki-dot_product]

[wiki-feature]: https://en.wikipedia.org/wiki/Feature_(machine_learning)
[wiki-tfidf]: https://en.wikipedia.org/wiki/Tf%E2%80%93idf
[wiki-dot_product]: https://en.wikipedia.org/wiki/Dot_product
